<!DOCTYPE html>
<html>
  <head>
    <link rel="stylesheet" type="text/css" href="styles.css" />
    <link
      href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&display=swap"
      rel="stylesheet"
    />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <title>Project 4</title>
  </head>

  <body class="pageContents">
    <h1>Project 4 - IMAGE WARPING and MOSAICING</h1>
    <div class="wrapper">
      <h2 class="title">Background of the project:</h2>
      <p class="introduction">
        In this project, we are asked to take the pictures that we like
        ourselves, and then warp one of the image into the other using the
        Homography computed by the chosen correspondeing points. And then
        blend/stitch the image together to get a mosaic of the images.
      </p>
      <h3>Part 1. Take the picture you like</h3>
      <p class="description">
        This seems like an easy task, but this task is really the root of having
        a good results. There are several things that you need to be careful
        when taking the picture:
      </p>
      <ol>
        <li>
          When you take the pictures, you need to make sure that all the
          pictures that you take have the same center of projection (COP). So
          when you take the pictures, you need to rotate your camera base on the
          axis of where the camera located. Not where you are standing, not the
          middle of the phone.
        </li>
        <li>
          The pictures that you take should have 40% ~ 70% of overlap between
          each consecutive picture for enough correspondency and easier
          registration
        </li>
        <li>
          It is best to take pictures of stationary scene, if you want to take
          picture of something moving in the scene, try to take each consecutive
          picture as fast as you cn to minimize the movement in each picture.
          One exception is that you want the effects for artistic reason.
        </li>
      </ol>
      <p class="imgTitle">Some Examples:</p>
      <div class="imageWrapper">
        <img src="images/cory529(2).jpg" class="resultImage" />
        <img src="images/cory529(3).jpg" class="resultImage" />
      </div>
      <div class="imageWrapper">
        <img src="images/bedroom1.jpg" class="resultImage" />
        <img src="images/bedroom2.jpg" class="resultImage" />
      </div>
      <div class="imageWrapper">
        <img src="images/kitchen1.jpg" class="resultImage" />
        <img src="images/kitchen2.jpg" class="resultImage" />
      </div>
      <hr />
      <h3>Part 2. Defining Correspondences</h3>
      <p class="description">
        Just like what we did in project 3. In order for the warping to work, we
        need consistent labeling point on both images. Which we need to define
        the correspondence manually on different items and elements in the
        images
        <br /><br />
        In this case, I tried to use barebone
        <a
          href=" https://www.geeksforgeeks.org/matplotlib-pyplot-ginput-in-python/"
          >ginput()</a
        >
        funtion at first, which works fine and it only required several lines of
        code. The problem is, you will need to redo the whole choosing process
        if you make mistakes in the middle. And you also need to write extra
        code to save the correspondeing points into a json file for future use.
        So I instead just used
        <a href="https://cal-cs180.github.io/fa23/hw/proj3/tool.html">this</a>
        labeling tool that is provided in the project 3 assignment specs for
        labeling the correspondence. And it provide the feature that allow the
        users to delete any points they chose during the choosing process, which
        is really convenient. It automatically save the correspondeing points
        into a json file as well.
      </p>

      <p>
        <b class="note">Note:</b> For the next part, you need to choose at least
        4 points to compute the Homography. And if you are taking your picture
        using iphone, which I did. I will suggest you should shrink the image
        for choosing points and for later warping and blending as well. This is
        because the original picture taken by iphone is too big, it will then
        cause your warping and blending a lot of time.
      </p>
      <p>
        <b class="suggestion">Suggestion:</b> Generally. more correspondency is
        better. But if you have some good pictures for easy registration and
        correspondency, you can still get a good results with less points (at
        least 4!)
      </p>

      <p class="imgTitle">Corresponding Points Examples:</p>
      <div class="imageWrapper">
        <img
          src="images/resized with scatter points cory529(2).jpg"
          class="resultImage"
        />
        <img
          src="images/resized with scatter points cory529(3).jpg"
          class="resultImage"
        />
      </div>
      <hr />
      <h3>Part 3. Compute Homography</h3>
      <p class="description">
        How can we warp one image into alignment of the other image? Which we
        will need to compute the Homoraphy between 2 images' correspondency that
        we chose in the previous part. Where H (Homography) is a 3 by 3 matrix
        with 8 degree of freedom (last element is scale). Here are the hand
        written notes of mine about how to compute the Homogeaphy base on the
        correspondency points:
      </p>

      <p class="imgTitle">Homogeaphy details:</p>
      <div class="noteImageWrapper">
        <img src="images/Homography note1.jpeg" class="noteImage" />
        <img src="images/Homography note2.jpeg" class="noteImage" />
        <img src="images/Homography note3.jpeg" class="noteImage" />
      </div>
      <p class="description">
        After you have the above equations setup, you can solve the matrix
        easily by using least square solution
      </p>
      <p>
        <b class="note">Note:</b> We can now see why we need at least 4
        correspondencies to solve a homograhpy. And the above equations are only
        for 1 correspondency. For more Correspondencies, just stack more left
        most matrix with different x1, x2, y1, y2 vertically.
      </p>
      <p>
        <b class="suggestion">Suggestion:</b> For sanity check, you can call one
        of the functions that is prohibited (ex. cv2.findHomography) as stated
        in project spec to see whether the H you computed is correct or not. It
        is normal that the results you have will be a little different than the
        one you get from the function. But don't worry about it if it is really
        small.
      </p>
      <hr />
      <h3>Part 4. Warp Image</h3>
      <p class="description">
        Now we know how to compute the Homography, we can use the Homography
        that we computed to warp an image to the reference image. Here are my
        hand written notes about how to wap image:
      </p>
      <p class="imgTitle" id="warpingNotes">warping details:</p>
      <div class="noteImageWrapper">
        <img
          src="images/warping note1.jpeg"
          class="noteImage"
          id="warpingNote1"
        />
        <img
          src="images/warping note2.jpeg"
          class="noteImage"
          id="warpingNote2"
        />
        <img
          src="images/warping note3.jpeg"
          class="noteImage"
          id="warpingNote3"
        />
      </div>

      <p>
        <b class="note">Note:</b> In the warping function, whenever you apply
        homography or inverse homograhpy, you need to devide the warped
        coordinates by the corresponding scale w (which is the w shown in the
        above homograhpy section).
      </p>
      <p>
        <b class="suggestion">Suggestion:</b> Just like the homography, you can
        call something like cv2.warpPerspective for sanity check. Which the
        result may be different because the function may cut off the image if
        there are negative coordinates after warping. But the function I
        implement shows the whole warped image
      </p>
      <hr />
      <h3>Part 5. Image Rectification</h3>
      <p class="description">
        Here comes the part where you can test whether your code for homograhpy
        and warping are correct or not (if you didn't use the sanity check).
        What we need to do here is bascially take a picture with some rectangle
        objects in the scene and define a rectangle area by yourself so that you
        can compute the homography base on the points you choose around the
        rectnagle objects in the scenes and the rectangle area you define
        yourself (I used [[0, 0], [200, 0], [200, 200], [0, 200]] here myself).
      </p>
      <p class="imgTitle">My result:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Original Image:</p>
          <img src="images/ipad.jpg" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Rectified Image:</p>
          <img src="images/rectified ipad.jpg" class="resultImage" />
        </div>
      </div>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Original Image:</p>
          <img src="images/kurumi2.jpg" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Rectified Image:</p>
          <img src="images/rectified kurumi.jpg" class="resultImage" />
        </div>
      </div>
      <p class="description">
        You can now observe that, the result image is just like looking right
        into the object instead of from the side in the original image.
      </p>
      <p>
        <b class="note">Note:</b>
        If your result image is not stable or not good enough, you can try to
        choose more points on the rectangle instead of just using the 4 corners
        like I did. And if your original image is taken from a really off angle,
        your result image will look good only in a small portion of the image,
        all other parts will look really stretched. You can crop the image for
        only the good part for the result.
      </p>
      <p>
        <b class="suggestion">Suggestion:</b> if you know your original
        rectangle object's shape (height, width proportion), you can also define
        your rectangle into the same proportion for a better result (like
        instead of using a 200 * 200 square, I should probably use some other
        rectangle shape if I kow the exact proportion since ipad is not a
        square, but this is not requied).
      </p>
      <hr />
      <h3>Part 6. Blend the images into a mosaic</h3>
      <p class="description">
        After you got your rectification working, which means your codes for
        homograhpy and warping are working as well. We can now get into the main
        idea of this ptoject (project 4a), which is image mosaic. The idea is
        really easy, you first warp one image to the reference image. and blend
        the warped image and the reference image into a big mosaic. The idea of
        warping image is the same as image rectification, but instead of using a
        rectangle that you define yourself, you use the correspondency points to
        compute homography. And for the blending part, I just use my code from
        project 2 and tweak it a little to blend the image using the mask I
        create.
      </p>
      <p class="imgTitle">My result:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Original Image 1:</p>
          <img src="images/cory529(2).jpg" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Original Image 2:</p>
          <img src="images/cory529(3).jpg" class="resultImage" />
        </div>
      </div>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Blended Image:</p>
          <img src="images/Blend of cory.jpg" class="resultImage" />
        </div>
      </div>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Original Image 1:</p>
          <img src="images/bedroom1.jpg" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Original Image 2:</p>
          <img src="images/bedroom2.jpg" class="resultImage" />
        </div>
      </div>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Blended Image:</p>
          <img src="images/Blend of bedroom.jpg" class="resultImage" />
        </div>
      </div>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Original Image 1:</p>
          <img src="images/kitchen1.jpg" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Original Image 2:</p>
          <img src="images/kitchen2.jpg" class="resultImage" />
        </div>
      </div>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Blended Image:</p>
          <img src="images/Blend of kitchen.jpg" class="resultImage" />
        </div>
      </div>
      <p>
        <b class="note">Note:</b>
        Since when we warp one image to the reference image, there are some
        translation happening (like mentioned in the above section). We need to
        keep track of that translation and apply that to the reference image as
        well when perform blending.
      </p>
      <p>
        <b class="suggestion">Suggestion:</b> Although alpha blending (channel)
        is also mentioned in the project spec, I think Laplacian pyramid
        blending generally performs better for blending and easier to implement
        since you have went through that already in project 2. You probably just
        need a little modification in the code.
      </p>
      <h4>Some general Q & A about Blending</h4>
      <p>
        <b class="note">Q:</b>
        How do you define the overall output size for the mosaic?
      </p>
      <p>
        <b class="suggestion">A:</b>
        Which is a really similar idea as how you determine the
        <a href="#warpingNotes">ouput size for a warped image</a>. But in this
        case, instead of considering only 4 warped points (corners) of the
        warped image, you need to consider the translation and the 4 points
        (corners) of the reference image as well. Here are some example
        pictures:
      </p>
      <p class="imgTitle" id="canvases">
        Example points you need to consider for the ouput mosaic size:
      </p>
      <div class="imageWrapper">
        <img src="images/output_size example 1.jpg" class="resultImage" />
        <img src="images/output_size example 2.jpg" class="resultImage" />
      </div>
      <p class="description">
        Just like image warping, you now find the min_x, min_y, max_x, max_y
        using all the points and define the output mosaic size.
      </p>
      <p>
        <b class="note">Q:</b>
        How do you create the irregular mask for blending?
      </p>
      <p>
        <b class="suggestion">A:</b>
        For this part, I personally think it is a little bit hard to understand.
        But in general, we need to use some method like
        <a
          href="https://www.geeksforgeeks.org/python-opencv-distancetransform-function/"
          >cv2.distanceTransform()</a
        >
        to get the distance transform for both pictures. And use them to create
        the mask by comparing 2 distance transform image.
      </p>
      <p class="imgTitle">Example distance transform images:</p>
      <div class="imageWrapper">
        <img src="images/example dis transform1.jpg" class="resultImage" />
        <img src="images/example dis transform2.jpg" class="resultImage" />
      </div>
      <p class="imgTitle">Example masks for blending</p>
      <div class="imageWrapper">
        <img src="images/example mask1 greater.jpg" class="resultImage" />
        <img src="images/example mask2 greater.jpg" class="resultImage" />
      </div>
      <p class="description">
        What we do here is bascially just compare the 2 distance transform image
        to create the masks. At a given pixel, if distance transform on the left
        is greater than the distance transform on the right, we assign the the
        mask pixel to be 1, otherwise 0. The other mask the same lgoic, but for
        distance transform on the right is greater than the left. After you get
        the masks, you can assign one of the mask to be the reverse mask in
        Laplacian blending, and blend them exactly like how we did in project2.
      </p>
      <p>
        <b class="note">Note: </b>
        As you can see, the distance transform image is based on the entire
        output mosaic size with only one of the image instead of just the images
        itself. The original images that you feed into cv2.distanceTransform()
        should be something like <a href="#canvases">this</a> for creating the
        mask.
      </p>
      <hr />
    </div>
    <h1>Part B - FEATURE MATCHING for AUTOSTITCHING</h1>
    <div class="wrapper">
      <h2 class="title">Background of the project:</h2>
      <p class="introduction">
        In the second part of this project, we are technically doing the exact
        same thing as part A in general, but we now implement the entire process
        to be autonomous so that we don't need to define the correspondency
        ourselves manually. For more details about the algorithm and logic that
        we are implementing, please refer to this
        <a
          href="https://inst.eecs.berkeley.edu/~cs180/fa24/hw/proj4/Papers/MOPS.pdf"
          >paper</a
        >
        that is provided in the project spec. Note that you may find out that we
        skip some details here in our own implementation.
      </p>
      <h3>Part 1. Detecting corner features in an image</h3>
      <p class="description">
        As the first step, we need to find all the potential points in both
        images (or more if you want to do a larger mosaic) that may be served as
        the correspondency. Over here, we implement a
        <a href="https://en.wikipedia.org/wiki/Harris_corner_detector"
          >Harris corner detector</a
        >
        to detect all the corners in the image. Luckily, we are provided with
        some
        <a href="https://inst.eecs.berkeley.edu/~cs180/fa24/hw/proj4/harris.py"
          >starter code</a
        >
        that we don't need to implement the harris corner detector ourselves.
        Here are the results:
      </p>
      <p class="imgTitle">Harris Corner Example</p>
      <div class="imageWrapper">
        <img
          src="images/resized with harris corners kitchen1.jpg"
          class="resultImage"
        />
        <img
          src="images/resized with harris corners kitchen2.jpg"
          class="resultImage"
        />
      </div>
      <p>
        <b class="note">Note: </b>
        Since the Harris corner method that provided to us only accept an image
        with one channel, so please remember to grayscale your image before
        throwing that into the harris corner function call.
      </p>

      <p class="description">
        But as you can porbably tell, there are too many possible points that we
        can choose right now, and we don't need all these points. In order to
        deal with the issue, we implement ANMS algorithm to get the points that
        are strong and also distribute across the entire image.
      </p>
      <p class="imgTitle">Corners after ANMS Example (300 points):</p>
      <div class="imageWrapper">
        <img
          src="images/ANMS scatter points kitchen1.jpg"
          class="resultImage"
        />
        <img
          src="images/ANMS scatter points kitchen2.jpg"
          class="resultImage"
        />
      </div>
      <p class="description">
        The idea of ANMS is really easy. Basically, all the points that you see
        in the images right now are the "strongest" in the circle area with some
        radius of r. The strength is the h value that the provided harris corner
        method return corresponding to each coordinates. And we find all the r
        that satisfy the following formula: \( r_i = \min_{j} \, \lvert
        \vec{x}_i - \vec{x}_j \rvert \;, s.t. h(\vec{x}_i) < c_{\text{robust}}
        \, h(\vec{x}_j) \). And \(c_{\text{robust}} = 0.9\) (specified in the
        paper). After you find a r for all points, you can now sort the
        coordinates in descending order (from high to low) using r, and specify
        how many points you want. In my case here, I have 300 points. And you
        can see that the points are well distributed over the entire image.
      </p>
      <p>
        <b class="note">Note: </b>
        You can vectorize the calculation using the dist2() method that is given
        in the same starter code as the harris corner detector. Which will be a
        lot faster than the navie idea of using 2 nested for loops. But I
        personally do the latter since it makes more sense to me. And it takes
        roughly 2 ~ 3 min to run for the 2 images combined.
      </p>
      <hr />
      <h3>Part 2. Extracting a Feature Descriptor for each feature point</h3>
      <p class="description">
        After we have the potential points, we can now start to find the
        correspondency. But beofre that, we need to implement feature descriptor
        to get the image patches around each coordinates so that we can compare
        each patches to see whether they are the match or not. The idea is easy,
        we first blur the entire image for anti-aliasing. And for each points,
        we extract the 40 * 40 pixels area around it as a patch, and resize the
        patch to 8 * 8. For every patch, we need to bias/gain-normalize them.
      </p>
      <p class="imgTitle">Example patches after normalization:</p>
      <div class="imageWrapper">
        <img src="images/example patches1.jpg" class="resultImage" />
        <img src="images/example patches2.jpg" class="resultImage" />
      </div>
      <p>
        <b class="note">Note: </b>
        As you can see, I grayscale my patches here like mentioned in the paper,
        but I also heard a lot of cases where keep the RGB produces better
        result, so you can try both to see which is better. And the patches you
        see in jupyter notebook should be more pixelated than what you see here.
        It is because plt.imsave() do somethng werid to smooth things out.
      </p>
      <hr />
      <h3>Part 3. Matching these feature descriptors between two images</h3>
      <p class="description">
        Since we already have all the patches we need, we can now match the
        patches by calculating euclidean distance between them. But instead of
        setting a threshold on the closest match, we use the "Lowe's trick"
        where we set a threshold on the ratio of first nearest neighbor error
        over the second nearest neighbor error (error here is the euclidean
        distance). This is because the error of the correct matches should be
        significantly lower than the incorrect matches. But the sclae of error
        varies a lot across the image depends on the appearance. So the Lowe's
        trick is more robust here.
      </p>
      <p class="imgTitle">Example Feature Matching:</p>
      <div class="imageWrapper">
        <img src="images/example feature matching.jpg" class="noteImage" />
      </div>
      <p class="description">
        As we can see, the matches are all correct and we have a lot less points
        compare to what we have at first. But if you compute the homography
        using all the points right now, it may still be off. (Sometime it may be
        close enough that you can just warp the image right away, but it is not
        robust). There are still some outliers, and we will get rid off them in
        the next section.
      </p>
      <p>
        <b class="note">Note: </b>
        Although I didn't do it, but for different set of images, you may need
        to adjust the threshold of the Lowe's trick.
      </p>
      <hr />
    </div>
  </body>
</html>
